[
  {
    "objectID": "04-dplyr.html",
    "href": "04-dplyr.html",
    "title": "dplyr",
    "section": "",
    "text": "Import data into a tidy structure\nFormat, filter and manipulate your datasets in preparation for plotting\nUnderstand the reason and methods for long and wide data formats\nThe dplyr package is specifically designed for data formatting and manipulation and allows you to merge datasets and create new columns as well as filtering and summarising your data. We are going to learn some of the most common functions."
  },
  {
    "objectID": "04-dplyr.html#selecting-columns-and-filtering-rows",
    "href": "04-dplyr.html#selecting-columns-and-filtering-rows",
    "title": "dplyr",
    "section": "Selecting columns and filtering rows",
    "text": "Selecting columns and filtering rows\nFirst, make sure the tidyverse package is loaded and you have read in the surveys dataset.\n\nlibrary(tidyverse)\nsurveys &lt;- read_csv(\"http://bifx-core3.bio.ed.ac.uk/training/R_dplyr_and_ggplot2/data/surveys_complete.csv\")\n\nTo select columns of a tibble, use select(). The first argument to this function is the data frame surveys, and the subsequent arguments are the columns to keep.\n\nselect(surveys, plot_id, species_id, weight)\n\nTo drop columns from a tibble, put a “-” in front of the variable to exclude it.\n\nselect(surveys, -record_id, -species_id)\n\nThis will select all the variables in surveys except record_id and species_id.\nTo choose rows based on specific criteria, use filter():\n\nfilter(surveys, year == 1995)"
  },
  {
    "objectID": "04-dplyr.html#pipes",
    "href": "04-dplyr.html#pipes",
    "title": "dplyr",
    "section": "Pipes",
    "text": "Pipes\nWhat if you want to select and filter at the same time? There are three ways to do this:\n\nIntermediate objects\nNested functions\nPipes\n\nWith intermediate objects, you create a temporary tibble and use that as input to the next function, like this:\n\nsurveys2 &lt;- filter(surveys, weight &lt; 5)\nsurveys_sml &lt;- select(surveys2, species_id, sex, weight)\n\nThis is readable, but can clutter up your workspace with lots of objects that you have to name individually and keep track of.\nYou can also nest functions (i.e. one function inside of another), like this:\n\nsurveys_sml &lt;- select(filter(surveys, weight &lt; 5), species_id, sex, weight)\n\nThis is handy, but can be difficult to read if too many functions are nested. R evaluates the expression from the inside out (in this case, filtering, then selecting).\nThe alternative is to use pipes. Pipes let you take the output of one function and send it directly to the next, which is useful when you need to do many things to the same dataset. Pipes in R look like %&gt;% or |&gt;. If you use RStudio, the keyboard shortcut for a pipe is Ctrl + Shift + M.\n\nsurveys |&gt;\n  filter(weight &lt; 5) |&gt;\n  select(species_id, sex, weight)\n\nHere we are ‘piping’ the surveys dataset through the filter() function, then through select(). Since |&gt; takes the object on its left and passes it as the first argument to the function on its right, we don’t need to explicitly include the tibble as an argument to the filter() and select() functions any more.\nSome may find it helpful to read the pipe like the word “then”. For instance, in the above example, we took the data frame surveys, then we filtered for rows with weight &lt; 5, then we selected columns species_id, sex, and weight. The dplyr functions by themselves are somewhat simple, but by combining them into linear workflows with the pipe, we can accomplish more complex manipulations of data.\nIf we want to create a new object with this smaller version of the data, we can assign it a new name:\n\nsurveys_sml &lt;- surveys |&gt;\n  filter(weight &lt; 5) |&gt;\n  select(species_id, sex, weight)\nsurveys_sml\n\n\n\n Further Learning\n\nPiping was originally introduced by the magrittr package which uses the %&gt;% symbol. Magrittr is part of the tidyverse and loads automatically with dplyr.\nR version 4.1 introduced a native pipe which can be used in base R without loading the magrittr or tidyverse packages. It uses the alternative |&gt; symbol. There are some subtle differences with this pipe which you can read about here. You can change the CTRL+ALT+M keyboard shortcut to use the native pipe in the Code section of RStudio’s Global Options.\n\n\n\n\n Challenge:\n\nUsing pipes, subset the surveys data to include animals collected before 1995 and retain only the columns year, sex, and weight.\n\n\n\n\nSolution. \n\n Solution:\n\n\nsurveys |&gt;\n  filter(year &lt; 1995) |&gt;\n  select(year, sex, weight)\n\n# A tibble: 18,044 × 3\n    year sex   weight\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;\n 1  1978 M        204\n 2  1978 M        199\n 3  1978 M        197\n 4  1979 M        166\n 5  1979 M        184\n 6  1979 M        206\n 7  1979 F        274\n 8  1979 F        186\n 9  1980 F        184\n10  1980 F         87\n# ℹ 18,034 more rows"
  },
  {
    "objectID": "04-dplyr.html#exporting-data",
    "href": "04-dplyr.html#exporting-data",
    "title": "dplyr",
    "section": "Exporting data",
    "text": "Exporting data\nSimilar to the read_csv() function there is also a write_csv() function. If you want to export any of your newly created datasets you can do this as follows:\n\n##Create a data folder\ndir.create(\"data\")\n\nwrite_csv(surveys_sml, file = \"data/surveys_small.csv\")\n#Or use write_tsv for tab separated files\n\nYou can even pipe directly into write_csv to avoid creating another R object.\n\nsurveys |&gt;\n  filter(weight &lt; 5) |&gt;\n  select(species_id, sex, weight) |&gt; \n  write_csv(file = \"data/surveys_small.csv\")"
  },
  {
    "objectID": "04-dplyr.html#mutate",
    "href": "04-dplyr.html#mutate",
    "title": "dplyr",
    "section": "Mutate",
    "text": "Mutate\nFrequently, you’ll want to create new columns based on the values in existing columns, for example to do unit conversions, or to combine values from two columns. For this we use mutate().\nTo create a new column of weight in kg:\n\nsurveys |&gt;\n  mutate(weight_kg = weight / 1000)\n\nYou can also create a second new column based on the first new column within the same call of mutate():\n\nsurveys |&gt;\n  mutate(weight_kg = weight / 1000,\n         weight_lb = weight_kg * 2.2)\n\nIf you just want to see the first few rows, you can use a pipe to one of the head commands. Base R has head() function and the Tidyverse uses slice_head() which is one of the useful slice functions for selecting specific rows of a dataframe or tibble.\n\nsurveys |&gt;\n  mutate(weight_kg = weight / 1000) |&gt;\n  head()\n\nsurveys |&gt;\n  mutate(weight_kg = weight / 1000) |&gt;\n  slice_head(n=6)\n\nYou can even use functions within mutate. Look at the code below and see if you can figure out how the case_when function works.\n\nsurveys |&gt;\n  mutate(sex = case_when(sex==\"M\"~\"Male\",sex==\"F\"~\"Female\"))\n\nThe stringr package within the Tidyverse contains a lot of useful functions for manipulating character strings. For instance, let’s imagine that the genus ‘Perognathus’ has been mislabeled in our table and should instead be ‘Peromyscus’.\n\nsurveys |&gt;\n  mutate(genus = str_replace(genus,\"Perognathus\",\"Peromyscus\"))\n\nWe can check this has worked by pulling the genus column, converting it to a factor and running a summary.\n\n# summarise the genus column in surveys\nsurveys |&gt;\n  pull(genus) |&gt; # pull extracts a single column\n  as.factor() |&gt; # convert character to factor (categorical variable)\n  summary()\n\n# Try again, replacing Perognathus with Peromyscus\nsurveys |&gt;\n  mutate(genus = str_replace(genus,\"Perognathus\",\"Peromyscus\")) |&gt; \n  pull(genus) |&gt; # pull extracts a single column\n  as.factor() |&gt; # convert character to factor (categorical variable)\n  summary()\n\n\n\n Challenge:\n\nCreate a new data frame from the surveys data that meets the following criteria: Contains only the species_id column and a new column called hindfoot_cm containing the hindfoot_length values converted to centimeters. In this hindfoot_cm column all values are less than 3.\nHint: think about how the commands should be ordered to produce this data frame!\n\n\n\n\nSolution. \n\n Solution:\n\n\nsurveys_hindfoot_cm &lt;- surveys |&gt;\n    mutate(hindfoot_cm = hindfoot_length / 10) |&gt;\n    filter(hindfoot_cm &lt; 3) |&gt;\n    select(species_id, hindfoot_cm)"
  },
  {
    "objectID": "04-dplyr.html#group_by-and-summarise",
    "href": "04-dplyr.html#group_by-and-summarise",
    "title": "dplyr",
    "section": "Group_by and summarise",
    "text": "Group_by and summarise\nMany data analysis tasks can be approached using the split-apply-combine paradigm: split the data into groups, apply some analysis to each group, and then combine the results. dplyr makes this very easy through the use of the group_by() function.\ngroup_by() is often used together with summarise(), which collapses each group into a single-row summary of that group. The group_by() function takes the column names that contain the grouping variables for which you want to calculate summary statistics. The summarise() function computes a summary statistic for each group using a specified function and variable. So to compute the mean weight by sex:\n\nsurveys |&gt;\n  group_by(sex) |&gt;\n  summarise(mean_weight = mean(weight))\n\nYou can also group by multiple columns:\n\nsurveys |&gt;\n  group_by(sex, species_id) |&gt;\n  summarise(mean_weight = mean(weight))\n\nOnce the data are grouped, you can create multiple summary columns at the same time. For instance, we could add columns indicating the minimum weight and mean hindfoot length for each species for each sex:\n\nsurveys |&gt;\n  group_by(sex, species_id) |&gt;\n  summarise(min_weight = min(weight),\n            mean_hindfoot_length = mean(hindfoot_length))\n\nIt is sometimes useful to rearrange the result of a query to inspect the values. For instance, we can sort on min_weight to put the lighter species first and :\n\nsurveys |&gt;\n  group_by(sex, species_id) |&gt;\n  summarise(min_weight = min(weight),\n            mean_hindfoot_length = mean(hindfoot_length)) |&gt;\n  arrange(min_weight)\n\nTo sort in descending order, we need to add the desc() function. If we want to sort the results by decreasing order of mean weight:\n\nsurveys |&gt;\n  group_by(sex, species_id) |&gt;\n  summarise(min_weight = min(weight),\n            mean_hindfoot_length = mean(hindfoot_length)) |&gt;\n  arrange(desc(min_weight))"
  },
  {
    "objectID": "04-dplyr.html#count",
    "href": "04-dplyr.html#count",
    "title": "dplyr",
    "section": "Count",
    "text": "Count\nWhen working with data, we often want to know the number of observations found for each factor or combination of factors. For this task, dplyr provides count(). For example, if we wanted to count the number of rows of data for each sex, we would do:\n\nsurveys |&gt;\n    count(sex) \n\nThe count() function is shorthand for something we’ve already seen: grouping by a variable, and summarising it by counting the number of observations in that group. In other words, surveys |&gt; count(sex) is equivalent to:\n\nsurveys |&gt;\n    group_by(sex) |&gt;\n    summarise(count = n()) #n() counts the size of each group\n\nIf we wanted to count a combination of factors, such as sex and species, we would specify the first and the second factor as the arguments of count():\n\nsurveys |&gt;\n  count(sex, species) \n\nWith the above code, we can proceed with arrange() to sort the table according to a number of criteria so that we have a better comparison. For instance, we might want to arrange the table above in (i) an alphabetical order of the levels of the species and (ii) in descending order of the count:\n\nsurveys |&gt;\n  count(sex, species) |&gt;\n  arrange(species, desc(n))\n\n\n\n Challenge:\n\nHow many animals were caught in each plot_type surveyed?\n\n\n\n\nSolution. \n\n Solution:\n\n\nsurveys |&gt;\n    count(plot_type) \n\n\n\nUse group_by() and summarise() to find the mean, min, and max hindfoot length for each species (using species_id). Also add the number of observations (hint: see ?n).\n\n\n\n\nSolution. \n\n Solution:\n\n\nsurveys |&gt;\n   group_by(species_id) |&gt;\n   summarise(\n       mean_hindfoot_length = mean(hindfoot_length),\n       min_hindfoot_length = min(hindfoot_length),\n       max_hindfoot_length = max(hindfoot_length),\n       n = n())\n\n\n\nWhat was the heaviest animal measured in each year? Return the columns year, genus, species_id, and weight. Order the output by year.\n\n\n\n\nSolution. \n\n Solution:\n\n\nsurveys |&gt;\n   group_by(year) |&gt;\n   filter(weight == max(weight)) |&gt;\n   select(year, genus, species, weight) |&gt;\n   arrange(year)\n\n\n\n\n\n\n\n Discussion\n\n\nLook at the surveys dataset with the print() function. How many rows and columns does it have?\nBy default, tibbles hide a lot of the data to prevent the screen from getting crowded. How can you show more rows and all of the columns?\nWhat other questions can we ask of this dataset? Set a challenge for a partner or your group using the dplyr functions."
  },
  {
    "objectID": "04-dplyr.html#pivoting-functions-in-tidyr",
    "href": "04-dplyr.html#pivoting-functions-in-tidyr",
    "title": "dplyr",
    "section": "Pivoting functions in tidyr",
    "text": "Pivoting functions in tidyr\nThe tidyr package contains the functions pivot_longer() and pivot_wider() which allow you to transform a dataset between long and wide formats. For instance, what if we wanted to compare the mean weights of each species at each plot (using plot_id).\nWe’d need to create a new table where each row is comprised of values of variables associated with each plot. In practical terms this means the values in genus would become the names of column variables and the cells would contain the values of the mean weight observed on each plot.\n\nHaving created a new table, it is then straightforward to explore the relationship between the weight of different genera within, and between, the plots. The key point here is that we are still following a tidy data structure,but we have reshaped the data according to the observation of interest. Mean genus weight per plot, instead of recordings per date.\nFirst, lets create surveys_gw where observations for each plot (genus and mean_weight) are spread across multiple rows:\n\nsurveys_gw &lt;- surveys |&gt;\n  group_by(plot_id, genus) |&gt;\n  summarise(mean_weight = mean(weight))\n\nUsing pivot_wider() with genus as the names of the new columns and mean_weight as the values, we can create a table that allows us to compare mean weights of different genera at each plot.\n\nsurveys_wide &lt;- surveys_gw |&gt;\n  pivot_wider(names_from = genus,values_from = mean_weight)\n\nThe opposing situation could occur if we had been provided with data in the form of surveys_wide, where the genus names are column names, but we wish to treat them as values of a variable instead. To do this we can use the pivot_longer() function.\n\n\n Further Learning\n\nExamples of more dplyr functions are available on the dplyr website.\n\n\n\n\n Key points\n\n\nImport and format data with readr and tidyr.\nUse dplyr select, filter and mutate to manipulate datasets\nUse group_by, summarise and count to create summary datasets by groups.\nUse pivot_longer and pivot_wider to move between wide and long formats."
  },
  {
    "objectID": "01-start.html",
    "href": "01-start.html",
    "title": "Getting Started",
    "section": "",
    "text": "We recommend using RStudio for this workshop. RStudio is an Integrated Development Environment (IDE) for R. It can be accessed in several ways."
  },
  {
    "objectID": "01-start.html#using-rstudio",
    "href": "01-start.html#using-rstudio",
    "title": "Getting Started",
    "section": "Using RStudio",
    "text": "Using RStudio\nA comprehensive RStudio user guide is available here.\n\n\nThe are 4 main panes, each with several tabs:\n\nConsole (bottom left)\n\nHere you can type commands into R\nAdditional tabs may include a terminal and script outputs\n\nSource (top left)\n\nOpen and view files\nThese can be raw txt, scripts or markdown\n\nEnvironments (top right)\n\nObjects you have stored\nCommands you have typed\nAdditional tabs for version control, database and website building…\n\nOutput (bottom right)\n\nSystem files (on the computer/server you are working on)\nOutput from plots or applications\nPackages available\nHelp pages\n\n\n\n\n\n\n\nYou can customise the appearance of RStudio under the Tools -&gt; Global Options… menu.\n\nSetting up a new project\nThere is a drop-down project menu at the top right of RStudio. Click this, select “New Project…” and create one in a new directory. Make sure you have write permission for the directory you choose.\nOnce you have done this, this will be your working directory. Files will be saved (or loaded from) here by default unless you specify a full path. You can change your working directory under the session menu at the top.\nUsing Rstudio has the advantage that everything you do can be saved between RStudio sessions."
  },
  {
    "objectID": "01-start.html#running-commands",
    "href": "01-start.html#running-commands",
    "title": "Getting Started",
    "section": "Running commands",
    "text": "Running commands\nYou can work in 3 different ways in RStudio.\n\nUse the console to run commands.\nCreate a new R script to save your commands as you go.\nCreate an R markdown file to generate web pages or pdf documents from your analyses.\n\nCommands can be typed directly into the console, but in order to keep track it’s best to write them into a script as you go (File-&gt;New File-&gt;R Script). From here you can use a shortcut to run the command on the line where your cursor is:\n\nAlt + Enter to keep the cursor on the same line\nCtrl + Enter to move to the next line\n\nYou can also use the Tab key to autocomplete names of functions and objects as you type them into your script.\nHint: When using the console, the Up/Down arrow keys can be use to cycle through previous commands.\nIn the console you should always see a &gt; prompt, if you can’t see this R may still be working. There is a red Stop light at the top right of the console when a command is running. If you see a + instead of &gt;, R is waiting for more input. Sometimes this means you have forgotten to close a bracket or quotation.\nUsing R Markdown is a great way to annotate your code and present an analysis with code, figures and text all together in a web page or document format. It’s worth learning but adds a further level of complication for novice users so is not covered in this course."
  },
  {
    "objectID": "01-start.html#installing-libraries",
    "href": "01-start.html#installing-libraries",
    "title": "Getting Started",
    "section": "Installing libraries",
    "text": "Installing libraries\nLibraries provide additional functions in R and can be downloaded from several sources:\n\nCRAN is the Comprehensive R Archive Network and hosts the majority of generic R packages.\nBioconductor is a repository of biology specific packages.\nThird party tools are often hosted on github.\n\nInstall the packages we need for these lessons by running the code below in the R console:\n\n#install from CRAN with install.packages()\ninstall.packages(c(\"tidyverse\", \"ggthemes\", \"ggrepel\", \"viridis\", \"RColorBrewer\", \"ggsci\", \"gghighlight\", \"plotly\", \"ggpubr\", \"rstatix\", \"patchwork\", \"cowplot\", \"ggiraph\"))\n\n#Example to install from bioconductor with BiocManager\n#if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n#BiocManager::install(c(\"DESeq2\",\"genomation\"))\n\n#Example to install from github with the devtools package\n#install.packages(\"devtools\")\n#devtools::install_github(\"thomasp85/patchwork\")\n\nTo load a specific package within an R session, use the library function:\n\nlibrary(tidyverse)\n\n\n\n How to follow this tutorial\n\n\nCreate a new project in RStudio\nInstall the required libraries\nOpen a new R script (or R markdown file for advanced users)\nIt will be best to work with the tutorial and RStudio open together so you can easily switch between the two. Working on a wide split-screen or multiple desktops is the best setup.\nI recommend typing out commands rather than copy-and-pasting if you want to learn. Remember you can use the Tab key to save yourself from endless typing!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to R Workshop",
    "section": "",
    "text": "This is the homepage for R training at the DRP-HCB Bioinformatics Core at the University of Edinburgh. These courses are run annually by the Bioinformatics Core but also available upon request.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor more information contact Shaun Webb."
  },
  {
    "objectID": "05-ggplot2.html",
    "href": "05-ggplot2.html",
    "title": "ggplot2",
    "section": "",
    "text": "Build graphics layer by layer with ggplot2\nCreate different types of graphics by applying geometries\nApply additional layers to a graphic\nVisually subset graphics by applying fills and gradients\nChange the appearance of graphics using themes\nCreate sub-graphics by applying facets\nCreate and save basic graphics\nUse additional libraries such as plotly to enhance the utility of graphics.\nggplot2 is a plotting package that makes it simple to create complex plots from data in a data frame. It provides a grammar for specifying which variables to plot, how they are displayed, and general visual properties. Therefore, we only need minimal changes to our code if the underlying data change or if we decide to switch from a bar plot to a scatterplot. This helps to create publication quality plots with minimal adjustments and tweaking.\nFirst, make sure you load all of the required libraries and datasets. There are quite a few new libraries here and we will explain them as we go.\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(plotly)\nlibrary(ggrepel)\nlibrary(ggpubr)\nlibrary(viridis)\nlibrary(RColorBrewer)\nlibrary(ggsci)\nlibrary(patchwork)\nlibrary(gghighlight)\n\n#This dataset is built in to R\ndata(iris)\n#Import from url\nsurveys &lt;- read_csv(\"http://bifx-core3.bio.ed.ac.uk/training/R_dplyr_and_ggplot2/data/surveys_complete.csv\")"
  },
  {
    "objectID": "05-ggplot2.html#fishers-iris-dataset-pre-installed-in-r",
    "href": "05-ggplot2.html#fishers-iris-dataset-pre-installed-in-r",
    "title": "ggplot2",
    "section": "Fisher’s Iris dataset (pre-installed in R)",
    "text": "Fisher’s Iris dataset (pre-installed in R)\n\nFisher examined the length and width of the petals and sepals in irises to determine a classification algorithm. Here we will explore the relationship between species and the dimension of the flowers via the ggplot2 plotting package. Fisher’s data is stored as a built in data set in R. To see the data type:\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\nThis data frame has two types of data, continuous and discrete.\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\nThe continuous data is of type number and the discrete data is of type factor, which can be one of the 3 species:\n\nlevels(iris$Species)\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nThe summary() function gives a summary of data in a column. For factors it will give us the count for each categorical variable.\n\nsummary(iris$Species)\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nWe can also use run summary() on an entire dataframe or tibble. Take a look at the different column types.\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50"
  },
  {
    "objectID": "05-ggplot2.html#building-a-ggplot",
    "href": "05-ggplot2.html#building-a-ggplot",
    "title": "ggplot2",
    "section": "Building a ggplot",
    "text": "Building a ggplot\nggplot graphics are built step by step by adding new elements. Adding layers in this fashion allows for extensive flexibility and customization of plots. To build a ggplot, we use the following basic template:\n\nggplot(data = &lt;DATA&gt;, mapping = aes(&lt;MAPPINGS&gt;)) + &lt;GEOM_FUNCTION&gt;()\n\nLet’s explore the iris dataset with ggplot. Use the ggplot() function and bind the plot to a specific dataframe using the data argument.\n\nggplot(data = iris)\n\n\n\n\n\n\n\n\nDefine a mapping (using the aesthetic (aes) function), by selecting the variables to be plotted and specifying how to present them in the graph, e.g. as x/y positions or characteristics such as size, shape, color, etc.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width))\n\n\n\n\n\n\n\n\nTo plot our data we need to add a geometry. Geoms are the graphical representations of the data in the plot (points, lines, bars). ggplot2 offers many different geoms, here are a few commonly used examples:\n\ngeom_point() for scatter plots, dot plots, etc.\ngeom_histogram() for histograms\ngeom_bar() or geom_col() for bar plots\ngeom_boxplot() for, well, boxplots!\ngeom_line() for trend lines, time series, etc."
  },
  {
    "objectID": "05-ggplot2.html#scatterplots",
    "href": "05-ggplot2.html#scatterplots",
    "title": "ggplot2",
    "section": "Scatterplots",
    "text": "Scatterplots\nTo add a geom to the plot use the + operator. Let’s use geom_point() to plot two continuous variables first. The data and mapping are always the first two arguments to ggplot so we can leave these out if arguments are provided in this order:\n\nggplot(iris,aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThe + in the ggplot2 package is particularly useful because it allows us to modify existing ggplot objects. This means we can easily set up plot templates and conveniently explore different types of plots, so the above plot can also be generated with code like this:\n\n# Assign plot to a variable\np &lt;- ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width))\n\n# Draw a scatter plot\np + geom_point()\n\n\n\n\n\n\n\n\nAnything you put in the ggplot() function is inherited by the subsequent geom layers that you add. This includes the x- and y-axis mapping you set up in aes(). If you want to specify mappings for a given geom independently you can add these within a geom.\nWe can colour the points in the geometry.\n\np + geom_point(colour=\"red\")\n\n\n\n\n\n\n\n\nNot very informative though, let’s start again and colour the points by species. Note as well that the x and y labels are also not neccessary if the data is in the correct order.\n\np&lt;-ggplot(iris, aes(Sepal.Length, Sepal.Width, colour=Species) ) +  geom_point()\np\n\n\n\n\n\n\n\n\nIn the aes() function, colour is used for lines and points. If you want to colour shapes such as bars in barplots and histograms you need to use fill.\nEach function within the ggplot package has its own help file associated with it.\n\n?geom_point()\n\nNote that all the geometries also define a statistical transformation. In the help file of geom_point you’ll see that stat = \"identity\" which is just the value that is presented. Other geometries have different defaults, for instance geom_histogram bins values together."
  },
  {
    "objectID": "05-ggplot2.html#histograms",
    "href": "05-ggplot2.html#histograms",
    "title": "ggplot2",
    "section": "Histograms",
    "text": "Histograms\nHere is an example of the histogram geometry:\n\n#Create a new version of the iris dataset with a new column\niris2 &lt;- iris |&gt; mutate(Sepal.Ratio=Sepal.Length/Sepal.Width)\n#Visualise the distribution of Sepal.Ratio with a histogram\nh&lt;- ggplot(iris2, aes(Sepal.Ratio, fill=Species )) +\n  geom_histogram(binwidth=0.1)\nh\n\n\n\n\n\n\n\n\n\n\n Discussion\n\nTry altering the binwidth parameter to see how this affects the plot.\n\n\n\n\n Challenge:\n\nUse what you have learned to create a scatterplot of Sepal.Width over Species.\n\n\n\n\nSolution. \n\n Solution:\n\n\nggplot(iris, aes(x=Species, y=Sepal.Width )) + geom_point()"
  },
  {
    "objectID": "05-ggplot2.html#boxplots",
    "href": "05-ggplot2.html#boxplots",
    "title": "ggplot2",
    "section": "Boxplots",
    "text": "Boxplots\nBoxplots are an effective way to visualise distributions of data within groups. The boxplot geometry needs the x value to be a categorical variable.\n\nb &lt;- ggplot(iris, aes(x=Species, y=Sepal.Width )) +  geom_boxplot() \nb\n\n\n\n\n\n\n\n\nTo change the axes we can use another function called coord_flip()\n\nb + coord_flip()\n\n\n\n\n\n\n\n\nWe can reorder the Species by the median Sepal.Width for display purposes. The forcats package in the tidyverse has lots of functions for ordering, labelling and manipulating factors (discrete / categorical variables).\n\nb &lt;- ggplot(iris, aes(x=fct_reorder(Species,Sepal.Width), y=Sepal.Width )) + \n  geom_boxplot()\nb\n\n\n\n\n\n\n\n\nWe probably want to relabel the x-axis now:\n\nb &lt;- b + xlab(\"Species\")\nb"
  },
  {
    "objectID": "05-ggplot2.html#adding-layers",
    "href": "05-ggplot2.html#adding-layers",
    "title": "ggplot2",
    "section": "Adding layers",
    "text": "Adding layers\nBy adding points to a boxplot, we can have a better idea of the number of measurements and of their distribution.\n\nb + geom_point(colour=\"forest green\")\n\n\n\n\n\n\n\n\nThis is okay, but there are some blurry dots where values overlap and it’s difficult to see how many data points we really have. We can use the alpha option within our geometry to increase transparency.\n\nb + geom_point(colour=\"forest green\",alpha=0.3)\n\n\n\n\n\n\n\n\nThis is slightly better, we can see that darker points have more overlap, but it isn’t great. geom_jitter is a good alternative to geom_point in this example as it shifts data points so that they don’t overlap. We will colour by Species so there is no confusion.\n\nb + geom_jitter(aes(colour=Species))"
  },
  {
    "objectID": "05-ggplot2.html#violin-plots",
    "href": "05-ggplot2.html#violin-plots",
    "title": "ggplot2",
    "section": "Violin plots",
    "text": "Violin plots\n\n\n Challenge:\n\nBoxplots are useful summaries, but hide the shape of the distribution. For example, if the distribution is bimodal, we would not see it in a boxplot. An alternative to the boxplot is the violin plot, where the shape of the density of points is drawn.\nReplace the box plot with a violin plot; see geom_violin().\n\n\n\n\nSolution. \n\n Solution:\n\n\nggplot(iris, aes(x=fct_reorder(Species,Sepal.Width), y=Sepal.Width, fill=Species )) + \n  geom_violin() +\n  xlab(\"Species\") \n\n\n\n\n\n\n\n\n\n\nCan you add a boxplot and datapoints over the violin plot?\n\n\n\n\nSolution. \n\n Solution:\n\n\nggplot(iris, aes(x=fct_reorder(Species,Sepal.Width), y=Sepal.Width)) + \n  geom_violin(aes(fill=Species),alpha=0.3)+ \n  geom_boxplot()+\n  geom_jitter(aes(colour=Species))+\n  xlab(\"Species\")"
  },
  {
    "objectID": "05-ggplot2.html#themes",
    "href": "05-ggplot2.html#themes",
    "title": "ggplot2",
    "section": "Themes",
    "text": "Themes\nRecap:\n\np&lt;-ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, colour=Species)) +  geom_point()\n\nWe can continue to modify this scatter plot. Let’s make the axis a bit prettier with the xlab() and ylab() functions:\n\np &lt;- p + xlab(\"Sepal length\") + ylab(\"Sepal width\") \np\n\n\n\n\n\n\n\n\nAll the characteristics of the plot, such as text size and the background are managed by the function called theme(). To see what a theme can change:\n\n?theme\n\nYou can use a theme with predefined defaults e.g:\n\np + theme_dark() #Dark theme\n\n\n\n\n\n\n\n#OR\np + theme_bw() #Black and white theme\n\n\n\n\n\n\n\n\nYou can also try theme_light, theme_minimal, theme_void, or create you own from scratch:\n\np + theme(\n            panel.background = element_blank(), \n            panel.grid.major = element_line(colour = \"darkgrey\"), \n            text = element_text(size=20), \n            axis.title.x=element_blank(), \n            axis.title.y=element_blank()\n            )\n\n\n\n\n\n\n\n# Note you can save your theme and reuse it \ntheme_for_nature &lt;- theme(\n            panel.background = element_blank(), \n            panel.grid.major = element_line(colour = \"darkgrey\"), \n            text = element_text(size=20), \n            axis.title.x=element_blank(), \n            axis.title.y=element_blank() \n            )\n\nWe can then reuse a theme, for example on the histogram we made earlier.\n\nh + theme_for_nature\n\n\n\n\n\n\n\n\nR-studio will offer help finding themes after typing “theme_”. Give some of them a try. The ggthemes and ggpubr packages also contain a selection of prebuilt themes e.g:\n\nlibrary(ggthemes)\np + theme_fivethirtyeight()\n\n\n\n\n\n\n\n\nWe can set a default theme to use in all ggplots:\n\ntheme_set(theme_bw())"
  },
  {
    "objectID": "05-ggplot2.html#adding-colours-to-plots",
    "href": "05-ggplot2.html#adding-colours-to-plots",
    "title": "ggplot2",
    "section": "Adding colours to plots",
    "text": "Adding colours to plots\nAs we have seen, points, lines and shapes can be coloured by a value in your dataframe using the aes() function. Earlier we coloured by a factor but we can also colour by a continuous value which will create a gradient of colour. Lets look at the Petals this time and colour by the ratio of Sepal.Length to Sepal.Width:\n\nq &lt;- ggplot(iris2, aes(x=Petal.Length, y=Petal.Width, colour=Sepal.Ratio)) +\n  geom_point(alpha=0.5)\nq\n\n\n\n\n\n\n\n\nTo change the colours we need to use the scale_color_* functions of which there are many. Some are built into ggplot2 and others exist in external packages such as viridis or ggthemes. The viridis and ColorBrewer palettes are particularly useful and worth investigating. The ggsci package contains palletes based on key scientific journals.\n\nContinuous scales\nTry the different scale options in the code below:\n\nlibrary(viridis)\nlibrary(RColorBrewer)\n#default\nq + scale_colour_continuous()\n\n\n\n\n\n\n\n#ColorBrewer palletes (many to choose from)\nq + scale_colour_distiller(palette = \"Spectral\")\n\n\n\n\n\n\n\n#Viridis colour scales (many to choose from)\nq + scale_colour_viridis()\n\n\n\n\n\n\n\n\nAn alternative method is to use the scale_colour_gradient* functions to define your own gradient e.g:\n\nq + scale_color_gradient2(high=\"darkred\", low=\"white\",  mid=\"red\", midpoint=2)\n\n\n\n\n\n\n\n\n\n\nDiscrete scales\nThere are also colour scales for discrete variables. Note that we must use fill for colouring boxes.\n\nbp&lt;- ggplot(iris2,aes(Species,Sepal.Length,fill=Species)) +\n  geom_boxplot()\n#Default\nbp + scale_fill_discrete()\n\n\n\n\n\n\n\n#Viridis\nbp + scale_fill_viridis(discrete = TRUE)\n\n\n\n\n\n\n\n#ColorBrewer Palettes\nbp + scale_fill_brewer(palette = \"Dark2\")\n\n\n\n\n\n\n\n#GGSci - Nature publishing group\nlibrary(ggsci)\nbp + scale_fill_npg()\n\n\n\n\n\n\n\n\nAlternatively you can manually choose your own colours:\n\nh + scale_fill_manual(values = c(\"forest green\",\"dodger blue\", \"firebrick\"))\n\n\n\n\n\n\n\n\n\n\n Further Learning\n\nThe options for colouring graphs are huge.\n\nColor brewer 2 is a great site for getting the hex values of colours to suit needs such as printer or colour blind friendliness.\nCoolors and Colormind are colour palette generators."
  },
  {
    "objectID": "05-ggplot2.html#adding-more-variables",
    "href": "05-ggplot2.html#adding-more-variables",
    "title": "ggplot2",
    "section": "Adding more variables",
    "text": "Adding more variables\nTake a look at this example:\n\nggplot(iris2, aes(x=Petal.Length, y=Petal.Width, colour=Sepal.Ratio)) +\n  geom_point()\n\n\n\n Challenge:\n\nSee if you can represent some of the other variables from the iris dataset in this plot as well. Hint: There are several other aesthetic mappings such as size and shape.\n\n\n\n\nSolution. \n\n Solution:\n\n\nggplot(iris2, aes(x=Petal.Length, y=Petal.Width, colour=Sepal.Ratio, shape=Species,size=Sepal.Length)) +\n  geom_point(alpha=0.5)"
  },
  {
    "objectID": "05-ggplot2.html#faceting",
    "href": "05-ggplot2.html#faceting",
    "title": "ggplot2",
    "section": "Faceting",
    "text": "Faceting\nWhen we have defined groups within a dataset, we can split our data into separate plots by using facets.\nLet’s look at plot q again\n\nq\n\n\n\n\n\n\n\n\nLet’s split this plot by species. This is achieved using either the facet_grid() or facet_wrap() functions:\n\nq + facet_wrap(~Species,nrow = 1,scales = \"fixed\")\n\n\n\n\n\n\n\n\nNote that the facet variable is preceded by the ~ character. This is used to define formulas in R. Try playing around with the nrow and scales parameters in facet_wrap().\nThe facet_grid() function is useful when you are splitting by multiple factors. For instance, let’s add a country column to iris2 by randomly assigning one of four nations of origin to each observation:\n\ncountries &lt;- c(\"Italy\", \"Spain\", \"France\", \"UK\")\n#Sample will randomly select a value for each row\niris2 &lt;- iris2 |&gt; mutate(Country=sample(countries,nrow(iris2),replace=TRUE))\n\nq2 &lt;- ggplot(iris2, aes(x = Petal.Length, y = Petal.Width, colour = Sepal.Ratio)) +\n  geom_point()\nq2 + facet_grid(Country ~ Species)\n\n\n\n\n\n\n\n\n\n\n Challenge:\n\nUse dplyr and ggplot to display the mean petal length for each species in each country except for those found in France. Use colouring and faceting to enhance the plot.\n\n\n\n\nSolution. \n\n Solution:\n\n\niris2 |&gt;\nfilter(Country != \"France\") |&gt; \ngroup_by(Species,Country) |&gt;\nsummarise(Mean=mean(Petal.Length)) |&gt; \nggplot(aes(x=Country, y=Mean, colour=Species)) + geom_point(size=4) + facet_wrap(~Species) + labs(title=\"Mean Petal Length\")"
  },
  {
    "objectID": "05-ggplot2.html#adding-labels",
    "href": "05-ggplot2.html#adding-labels",
    "title": "ggplot2",
    "section": "Adding labels",
    "text": "Adding labels\nYou can add labels to points in your graphs. One of the best ways to do this is to use the package ggrepel.\n\nlibrary(ggrepel)\n# you can use geom_text_repel() or geom_label_repel() to label whatever you want with non-overlapping labels. In the brackets use conditional subsetting to only label the interesting elements of your data.\n\nggplot(iris2,aes(Petal.Width,Petal.Length,colour=Sepal.Ratio)) +\n  geom_point() +\n  geom_label_repel(aes(label=ifelse(Petal.Length &gt; 6.4 ,as.character(Species),'')))\n\n\n\n\n\n\n\n\n\nHighlight points or lines\nYou can choose to highlight specific points or lines using gghighlight:\n\nlibrary(gghighlight)\nggplot(iris2,aes(Petal.Width,Petal.Length,colour=Sepal.Ratio)) +\n  geom_point() +\n  gghighlight(Petal.Length &gt; 6.4,label_key = Species)"
  },
  {
    "objectID": "05-ggplot2.html#saving-plots",
    "href": "05-ggplot2.html#saving-plots",
    "title": "ggplot2",
    "section": "Saving plots",
    "text": "Saving plots\nIn RStudio there are many options to save the image. However, if you are are wanting to use ggplot2 in a script, or via web interfaces, you can export a graph using the function ggsave().\n\n#save a png file \nggsave(\"IrisScatterplot.png\", p)\n#save a pdf file\nggsave(\"IrisScatterplot.pdf\", p)\n\nThe image format is automatically assigned from the file extension you use in the filename. Within ggsave() you can also set the resolution for the image as well as the length and width for the image. See the help page for ggsave for more options."
  },
  {
    "objectID": "05-ggplot2.html#arranging-multiple-plots",
    "href": "05-ggplot2.html#arranging-multiple-plots",
    "title": "ggplot2",
    "section": "Arranging multiple plots",
    "text": "Arranging multiple plots\nPlacing ggplots side by side can be performed with packages like gridExtra and patchwork. Patchwork uses simple formulas to arrange ggplots:\n\nlibrary(patchwork)\n#On top\nq/bp\n\n\n\n\n\n\n\n\n\n#Side by side\nq+bp\n\n\n\n\n\n\n\n\n\n#2 plots on top, 1 on bottom\n(q+h)/bp\n\n\n\n\n\n\n\n\n\n#Collect the guides together on one side\n(q+h)/bp + plot_layout(guides = \"collect\")\n\n\n\n\n\n\n\n\nUse ggsave to save the image:\n\nggsave(\"DoublePlot.png\",q/bp)"
  },
  {
    "objectID": "05-ggplot2.html#additional-functions-to-try",
    "href": "05-ggplot2.html#additional-functions-to-try",
    "title": "ggplot2",
    "section": "Additional functions to try",
    "text": "Additional functions to try\n\nCut functions\nThe cut_* functions in ggplot2 can turn continous data into discrete levels. In this example we create a new column that puts the data into 5 groups of equal size based on Sepal.Length:\n\niris2 &lt;- iris2 |&gt; mutate(Sepal.Length.Group=cut_number(Sepal.Length,5))\n\nggplot(iris2,aes(Sepal.Length,Sepal.Width,fill = Sepal.Length.Group)) +  \n  geom_violin() +\n  labs(fill=\"Sepal length intervals\", x=\"Sepal length\",y=\"Sepal width\")\n\n\n\n\n\n\n\n\nNote the use of the labs() function to change multiple labels at once.\nHave a look at the difference between the cut_number, cut_interval and cut_width functions.\n\n\nScaling data\nLet’s look at a larger dataset like the surveys_complete data we used earlier:\n\nggplot(surveys,aes(weight,hindfoot_length))+geom_point()\n\n\n\n\n\n\n\n\nFor axes with large values or very dispersed values it can be useful to rescale using a log axis. This can be done in two ways:\n\n#We can transform the values in our ggplot call:\nggplot(surveys,aes(log10(weight),hindfoot_length))+geom_point()\n\n\n\n\n\n\n\n\n\n#OR we can change the axis to a log scale\nggplot(surveys,aes(weight,hindfoot_length))+geom_point() +\n  scale_x_log10()\n\n\n\n\n\n\n\n\n\n\n Discussion\n\nWhat is the difference between these three plots?\n\n\n\nInteractive graphs with Plotly and ggiraph\nPlotly for ggplot2 is a browser-based charting library that converts ggplots into interactive visualisations:\n\nlibrary(plotly)\np2&lt;-ggplot(iris2,aes(x = Sepal.Length, y = Sepal.Width, colour = Species)) + geom_point()\ngp2&lt;-ggplotly(p2)\n\nBy default the mouse over text is what is mapped in the aesthetics. You can add a text aesthetic to add this to the tooltip text.\n\np2 &lt;- p2+ geom_point(aes(text = Country))\ngp2&lt;-ggplotly(p2)\n\nInteractive plotly graphics open in the Viewer pane in Rstudio and can be embedded in R Markdown documents. Alternatively you can save them as individual web pages using the htmlwidgets package:\n\n#View plotly\ngp2\n\n\n\n\n\n\nYou can save plotly objects as html web pages:\n\nhtmlwidgets::saveWidget(as_widget(gp2), \"plotly_image.html\")\n\nggiraph is another package that allows you to create interactive graphics. It is particularly useful for creating interactive plots for the web.\nInstead of geom_point() we use geom_point_interactive() from the ggiraph package. We then use the girafe() function to render the plot. The tooltip is set in the aesthetics.\n\nlibrary(ggiraph)\ngg &lt;- ggplot(iris2, aes(x = Sepal.Length, y = Sepal.Width,  colour = Species, tooltip = Country)) +\n  geom_point_interactive()\n\ngirafe(ggobj = gg)\n\n\n\n\n\n\n\nCreating detailed graphics with ggpubr\nThe ggpubr package provides wrapper functions for ggplot that make it easier to generate complex, publication ready graphics. It includes functions for running statistical tests and displaying the results in the plot. After using ggpubr, you will need to reset your theme with theme_set().\n\n# Violin plots with box plots inside\ndata(\"ToothGrowth\")\ndf &lt;- ToothGrowth\n\nmy_comparisons &lt;- list( c(\"0.5\", \"1\"), c(\"1\", \"2\"), c(\"0.5\", \"2\") )\n\nggviolin(df, x = \"dose\", y = \"len\", fill = \"dose\",\n         palette = c(\"#00AFBB\", \"#E7B800\", \"#FC4E07\"),\n         add = \"boxplot\", add.params = list(fill = \"white\"))+\n  stat_compare_means(comparisons = my_comparisons, label = \"p.signif\")+ # Add significance levels\n  stat_compare_means(label.y = 50)                                      # Add p-value \n\n\n\n\n\n\n\n\nThe next example additionally uses the library cowplot to add density plots within the margins of the main figure. Don’t worry if you get lost here, this is just a demonstration of what’s possible in R and beyond the scope of this course.\n\nlibrary(cowplot) \n# Main plot\npmain &lt;- ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species))+\n  geom_point()+\n  ggpubr::color_palette(\"jco\")\n# Marginal densities along x axis\nxdens &lt;- axis_canvas(pmain, axis = \"x\")+\n  geom_density(data = iris, aes(x = Sepal.Length, fill = Species),\n              alpha = 0.7, linewidth = 0.2)+\n  ggpubr::fill_palette(\"jco\")\n# Marginal densities along y axis\n# Need to set coord_flip = TRUE, if you plan to use coord_flip()\nydens &lt;- axis_canvas(pmain, axis = \"y\", coord_flip = TRUE)+\n  geom_density(data = iris, aes(x = Sepal.Width, fill = Species),\n                alpha = 0.7, linewidth = 0.2)+\n  coord_flip()+\n  ggpubr::fill_palette(\"jco\")\np1 &lt;- insert_xaxis_grob(pmain, xdens, grid::unit(.2, \"null\"), position = \"top\")\np2&lt;- insert_yaxis_grob(p1, ydens, grid::unit(.2, \"null\"), position = \"right\")\nggdraw(p2)\n\n\n\n\n\n\n\n\n\n\n Final Challenge:\n\nSpend the remaining time playing around with the different ggplot options and see if you can create an interesting and appealing visualisation with a dataset of your choice.\n\nUse these resources for inspiration!\n\nggplot cheatsheet\nR graph gallery\nggplot extensions\n\n\n\n\n\n Key points\n\n\nggplot2 builds graphics layer upon layer\nBind a dataset to your ggplot function and map values to visual aesthetics\nApply different geometries to create different graphics\nUse colours, fills, gradients, shapes etc. to represent multiple variables\nUse themes to alter the appearance of a graphic\nGenerate sublots with facetting\nSave your graphics with ggsave\nThere are many additional libraries that extend the functionality of ggplot"
  },
  {
    "objectID": "03-tidyverse.html",
    "href": "03-tidyverse.html",
    "title": "Tidyverse",
    "section": "",
    "text": "Understand the philosophy of Tidy Data\nGet to know some of the Tidyverse packages\nThe tidyverse is a suite of packages that includes libraries such as dplyr and ggplot2. These packages are designed for data science and share underlying principles, grammar and data structures. There are many ways to do the same thing in R, but following the philosophy of tidy data and using the tidyverse packages will keep your datasets organised and make analysis easier in the long run."
  },
  {
    "objectID": "03-tidyverse.html#tidy-data",
    "href": "03-tidyverse.html#tidy-data",
    "title": "Tidyverse",
    "section": "Tidy data",
    "text": "Tidy data\nData can be represented in many different ways across multiple tables but the tidyverse packages are specifically designed to work with tidy datasets. Tidy data conforms to the following criteria:\n\nEach variable has its own column\nEach row is a single observation\nEach value has its own cell\n\n\nThis is the optimal structure when working in R and provides consistency amongst your datasets. Getting your data into R and wrangling it into the correct format is always the first step in your analysis. Fortunately, the tidyr package contains many functions to tidy up your dataset.\nWe will start by reading in a dataset. The readr package has functions for importing data as tibbles. Tibbles are the tidyverse compatible version of an R dataframe. They have stricter formatting and allow you to perform grouping of variables as we will see in the next section.\n\nlibrary(tidyverse)\n\n#If you already have the data installed on your computer you can read from a file:\nsurveys &lt;- read_csv(\"data/surveys_complete.csv\")\n\n\n#Otherwise you can read from a URL\nsurveys &lt;- read_csv(\"http://bifx-core3.bio.ed.ac.uk/training/R_dplyr_and_ggplot2/data/surveys_complete.csv\")\n\n\n\n Discussion\n\n\nLook at the options available in the read_csv and compare this with the read.table function we saw earlier.\nWhat other readr functions can you find?\n\n\n\nThis dataset contains observations from a field survey of different organisms at different sites (plots). Let’s inspect the data.\n\n#Type an R objects name to print the contents\nsurveys\n\n# A tibble: 30,463 × 13\n   record_id month   day  year plot_id species_id sex   hindfoot_length weight\n       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;  &lt;dbl&gt;\n 1       845     5     6  1978       2 NL         M                  32    204\n 2      1164     8     5  1978       2 NL         M                  34    199\n 3      1261     9     4  1978       2 NL         M                  32    197\n 4      1756     4    29  1979       2 NL         M                  33    166\n 5      1818     5    30  1979       2 NL         M                  32    184\n 6      1882     7     4  1979       2 NL         M                  32    206\n 7      2133    10    25  1979       2 NL         F                  33    274\n 8      2184    11    17  1979       2 NL         F                  30    186\n 9      2406     1    16  1980       2 NL         F                  33    184\n10      3000     5    18  1980       2 NL         F                  31     87\n# ℹ 30,453 more rows\n# ℹ 4 more variables: genus &lt;chr&gt;, species &lt;chr&gt;, taxa &lt;chr&gt;, plot_type &lt;chr&gt;\n\n\n\n#Use the View function\nView(surveys)\n\n\n#We can look at the structure of the dataset\nstr(surveys)\n\nspc_tbl_ [30,463 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ record_id      : num [1:30463] 845 1164 1261 1756 1818 ...\n $ month          : num [1:30463] 5 8 9 4 5 7 10 11 1 5 ...\n $ day            : num [1:30463] 6 5 4 29 30 4 25 17 16 18 ...\n $ year           : num [1:30463] 1978 1978 1978 1979 1979 ...\n $ plot_id        : num [1:30463] 2 2 2 2 2 2 2 2 2 2 ...\n $ species_id     : chr [1:30463] \"NL\" \"NL\" \"NL\" \"NL\" ...\n $ sex            : chr [1:30463] \"M\" \"M\" \"M\" \"M\" ...\n $ hindfoot_length: num [1:30463] 32 34 32 33 32 32 33 30 33 31 ...\n $ weight         : num [1:30463] 204 199 197 166 184 206 274 186 184 87 ...\n $ genus          : chr [1:30463] \"Neotoma\" \"Neotoma\" \"Neotoma\" \"Neotoma\" ...\n $ species        : chr [1:30463] \"albigula\" \"albigula\" \"albigula\" \"albigula\" ...\n $ taxa           : chr [1:30463] \"Rodent\" \"Rodent\" \"Rodent\" \"Rodent\" ...\n $ plot_type      : chr [1:30463] \"Control\" \"Control\" \"Control\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   record_id = col_double(),\n  ..   month = col_double(),\n  ..   day = col_double(),\n  ..   year = col_double(),\n  ..   plot_id = col_double(),\n  ..   species_id = col_character(),\n  ..   sex = col_character(),\n  ..   hindfoot_length = col_double(),\n  ..   weight = col_double(),\n  ..   genus = col_character(),\n  ..   species = col_character(),\n  ..   taxa = col_character(),\n  ..   plot_type = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\n\n\n Further Resources\n\n\nThere are cheatsheets available for many tidyverse and rstudio packages that will help you to choose the correct functions.\nTake a look at these slides or www.tidyverse.org for more information on the tidyverse.\n\n\n\n\n\n Key points\n\n\nThe tidyverse is a suite of R packages\nStick to the principles and philosophy of tidy data\nUse the readr package to import data as tibbles\nUse further tidyverse packages to tidy, re-format and visualise data"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "02-IntroR.html",
    "href": "02-IntroR.html",
    "title": "Introduction",
    "section": "",
    "text": "What is R?\nUnderstand R datatypes\nUnderstand how to use functions in base R\nImport datasets into R\nInspect and format a dataset\nCreate basic graphics\nRun statistical tests"
  },
  {
    "objectID": "02-IntroR.html#what-is-r",
    "href": "02-IntroR.html#what-is-r",
    "title": "Introduction",
    "section": "What is R?",
    "text": "What is R?\nR is an extremely powerful programming language for working with data, applying statistics and creating publication ready graphics. In this lesson you will learn how to program in R and use the RStudio environment. We will cover the basics of the R syntax using its built in packages (base R), as well as importing data, creating simple graphics and running statistical tests."
  },
  {
    "objectID": "02-IntroR.html#why-is-r-useful",
    "href": "02-IntroR.html#why-is-r-useful",
    "title": "Introduction",
    "section": "Why is R useful?",
    "text": "Why is R useful?\n\nIt’s free!\nIt’s Powerful. There are many libraries for application specific tasks.\n\nBioconductor is a repository for bioinformatics R software.\nTidyverse packages for data science with a shared philosophy for storing, manipulating and visualising data.\n\nPresentation quality graphics\n\nSave as a png, pdf or svg\n\nGenerate reproducible and persistent results\n\nR commands and analyses can be saved for reproducible and persistent analyses.\nCreate automated scripts to replicate analysis on different datasets.\n\nRStudio provides an interactive environment for working in R.\nR markdown can generate documents to present your code, annotation and results in useful reports.\nShiny can produce interactive applications for exploratory data analysis."
  },
  {
    "objectID": "02-IntroR.html#r-terms-used-in-this-workshop",
    "href": "02-IntroR.html#r-terms-used-in-this-workshop",
    "title": "Introduction",
    "section": "R terms used in this workshop",
    "text": "R terms used in this workshop\n\nWorking directory\n\nThis is the directory used to store your data and results.\nIt is useful if it is also the directory where your input data is stored.\n\nData classes\n\nValues in R are assigned a class to help functions to interpret them. Some common classes are:\n\nnumeric: Numbers\ncharacter: Strings of text\nfactor: Labels for categorical data (e.g. species, sex)\n\n\nData Types\n\nVector\n\nA collection of values of one data type\nEquivalent to a column in a table\nItems in the vector can be named\nE.g. A collection of recorded mouse weights\n\nData Frame\n\nA table\nEssentially a collection of vectors as columns\nColumns can be different data types\nColumns must have the same size\nE.g. A table of mouse weights with columns (Mouse_ID, Sex, Weight)\n\nMatrix\n\nA table where columns and rows are related\nAll values must be the same data type\nCommonly used for correlation and heatmap analysis\nE.g. A table of RNA-seq expression levels where each row is a gene and each column is a different sample.\n\nList\n\nLists are collections of R objects\nEach item in the list has a unique index or name\nA list can contain items of different object types and classes (e.g single values, vectors, data frames, matrices, other lists…)"
  },
  {
    "objectID": "02-IntroR.html#r-syntax",
    "href": "02-IntroR.html#r-syntax",
    "title": "Introduction",
    "section": "R Syntax",
    "text": "R Syntax\nR is a functional programming language:\n\nNearly every command is the name of a function followed by parentheses.\nThe inputs to a function, including different options, are placed in the brackets.\nYou can use the Tab key to see the options available or use the help documentation for each function.\n\nTypical command structure:\n\nFunction(data, options, moreOptions)\n\nLet’s run a function on a real data set. The cars data set is built into base R. We can look at it by typing its name.\n\ncars\n\n   speed dist\n1      4    2\n2      4   10\n3      7    4\n4      7   22\n5      8   16\n6      9   10\n7     10   18\n8     10   26\n9     10   34\n10    11   17\n11    11   28\n12    12   14\n13    12   20\n14    12   24\n15    12   28\n16    13   26\n17    13   34\n18    13   34\n19    13   46\n20    14   26\n21    14   36\n22    14   60\n23    14   80\n24    15   20\n25    15   26\n26    15   54\n27    16   32\n28    16   40\n29    17   32\n30    17   40\n31    17   50\n32    18   42\n33    18   56\n34    18   76\n35    18   84\n36    19   36\n37    19   46\n38    19   68\n39    20   32\n40    20   48\n41    20   52\n42    20   56\n43    20   64\n44    22   66\n45    23   54\n46    24   70\n47    24   92\n48    24   93\n49    24  120\n50    25   85\n\n\nWe can see that this is a table of values. If we run the class function we will see that R recognises this as a data.frame.\n\nclass(cars)\n\n[1] \"data.frame\"\n\n\nNow let’s run a more useful function. The plot function in R is used for making basic graphs. Try plotting the speed column against the dist (stopping distance) column.\n\n## Use the options within the plot function to customise the output\nplot(cars, xlab=\"Car Speed (mph)\", ylab=\"Stopping Distance (ft)\")\n\n\n\n\n\n\n\n\nWe can use the = sign or &lt;- to store the output of a function as an object,\n\n## These statements are identical\nresult = Function(data, options, moreOptions)\nresult &lt;- Function(data, options, moreOptions)\n\nTry storing the output of the summary function on the cars data set. You will see the object sum_cars appear in your Environment tab.\n\nsum_cars = summary(cars)\n\nTo see what this object holds, just type its name.\n\nsum_cars\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\nNot all functions need arguments. For instance, the getwd which returns the path of your working directory.\n\ngetwd()\n\nIf you want to change the working directory you can use setwd(\"/path/to/new_directory\") or use the session menu in RStudio.\nHelp is also a function in R. The command below provides the help page for the function read.table\n\nhelp(read.table)\n\nWe can also search the help documentation using help.search. Let’s see if we can find a function for running a t-test.\n\nhelp.search(\"t test\")\n\nNOTE quotes are needed for strings (character text), they are not needed when referring to R data objects or function names.\nThere is a short cut for help, ?, which shows the help page for a function.\n\n# same as help(read.table)\n?read.table\n\n?? searches for help pages on functions, same as help.search(“phrase”)\n\n# same as help.search(\"t test\")\n??\"t test\"\n\nNow let’s read in some data of our own. Use the read.table function to read in the table hosted at the URL below.\n\nread.table(\"http://bifx-core3.bio.ed.ac.uk/data.tsv\")\n\nR reads this table as a data.frame object and prints it to the R console by default. To save the table, we need to assign it to an object.\n\nmydata  &lt;- read.table(\"http://bifx-core3.bio.ed.ac.uk/data.tsv\")\n\nHere, mydata is an object name and the syntax &lt;- assigns the output of the function to it. Remember you can also use =.\n\n\n Key points\n\n\nData classes: Understand different data classes (numeric, character, factor)\nR objects: Understand different ways to structure data in R (vectors, dataframes, matrices, lists). There are several other types of R object.\nFunctions: Code in R is run within functions.\nHelp: Use the help features to find out how a function works."
  },
  {
    "objectID": "02-IntroR.html#getting-data-into-r",
    "href": "02-IntroR.html#getting-data-into-r",
    "title": "Introduction",
    "section": "Getting data into R",
    "text": "Getting data into R\nFor a beginner, this can be the hardest part, it is also the most important part to get right.\nIt is possible to create a vector by typing data directly into R using the function c. Think of it as a concatenate or combine function.\n\nx   &lt;-  c(1,2,3,4,5)\n\nThis creates a vector named ‘x’ which stores the numbers 1 - 5.\nYou can see what is in an object at any time by typing its name:\n\nx\n\n[1] 1 2 3 4 5\n\n\nCharacter values need to be quoted, otherwise R will look for a data object of that name.\n\ndaysofweek &lt;- c(\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\")\n\nYou can create data frames from vectors using the data.frame function:\n\ntable &lt;- data.frame(Index=x,Day=daysofweek)\n\nUsually however, you will want to input data from a file. You can read files on your computer or from a URL. We have touched on the read.table function already.\n\nmydata &lt;- read.table(\"http://bifx-core3.bio.ed.ac.uk/data.tsv\")\n\nR stores mydata as a data frame, containing multiple vectors.\n\nclass(mydata)\n\n[1] \"data.frame\"\n\n\nWe can look at our table by typing its name, but this prints a lot of rows. Using the head() function will only print the first few lines;\n\nhead(mydata, n=5)\n\n  V1 V2 V3 V4 V5\n1  A  B  C  D  E\n2  1  4  1  1  1\n3  2  5  1  1  2\n4  2  5  1  1  2\n5  3  6  1  2  3\n\n\nYou can also use the View() command to open data frames in the file pane.\nHmmm, something isn’t right with our rows here…\nBy default the read.table function assumes certain things from the file\n\nThe file is a plain text file (there are separate functions to read excel files)\nColumns are separated by any number of tabs or spaces\nThere are the same number of data points in each column\nThere is no header row (labels for the columns)\nThere is no column with names for the rows\n\nIf any of these are FALSE, we need to tell that to the function. If it has a header column use the ‘header=TRUE’ argument.\n\nmydata &lt;- read.table(\"http://bifx-core3.bio.ed.ac.uk/data.tsv\", header=TRUE)  # header=T also works\n\nNote the comma between different parts of the functions arguments.\nThis overwrites our previous table and now we should have headers.\n\nhead(mydata, n=5)\n\n  A B C D E\n1 1 4 1 1 1\n2 2 5 1 1 2\n3 2 5 1 1 2\n4 3 6 1 2 3\n5 3 6 1 2 3\n\n\nEach column can be identified by the using the $ sign (mydata$A mydata$B etc.)\nIf any of these are typed it will print to screen:\n\nmydata$A\n\n [1] 1 2 2 3 3 3 3 3 4 4 4 4 4 4 4 4 4 5 5 5 5 6 6 6 7 7 8\n\n\n\nOther options for read.table\nUse sep= to define how columns are separated in your input file. This file uses the tab character which we can write as “\\t”.\n\nmydata  &lt;- read.table(\"http://bifx-core3.bio.ed.ac.uk/data.tsv\", header=T, sep=\"\\t\")\n\nBy default, read.table assumes columns are separated by any amount of white space (space or tabs). This can lead to problems if some of your columns have missing values, so it is good practice to always give a sep argument.\nIf your data has missing values you can use fill=TRUE.\n\nmydata  &lt;- read.table(\"http://bifx-core3.bio.ed.ac.uk/data.tsv\", header=T, sep=\"\\t\", fill=TRUE)\n\nThis causes R to fill empty spaces in columns with the ‘NA’ character.\nAs this is such a common task there are functions identical to read.table but with different default settings. e.g. read.delim and read.csv. Check out the help pages for each.\n\n\nImporting Datasets\nIn the Environment pane in RStudio there is a button called “Import Dataset”. This can make importing data much easier and calls the read.* set of functions for you. The command used will be displayed on the console. Note that you need to have the file on the computer to use this button."
  },
  {
    "objectID": "02-IntroR.html#inspecting-and-subsetting-a-dataset",
    "href": "02-IntroR.html#inspecting-and-subsetting-a-dataset",
    "title": "Introduction",
    "section": "Inspecting and subsetting a dataset",
    "text": "Inspecting and subsetting a dataset\nLet’s use some simple functions to inspect and summarise our data.\n\nsummary(mydata) # Summary of the whole data frame \n\n       A               B                C               D        \n Min.   :1.000   Min.   : 4.000   Min.   :1.000   Min.   :1.000  \n 1st Qu.:3.000   1st Qu.: 6.000   1st Qu.:1.000   1st Qu.:2.000  \n Median :4.000   Median : 7.000   Median :1.000   Median :3.000  \n Mean   :4.296   Mean   : 7.296   Mean   :1.778   Mean   :3.333  \n 3rd Qu.:5.000   3rd Qu.: 8.000   3rd Qu.:2.000   3rd Qu.:4.000  \n Max.   :8.000   Max.   :11.000   Max.   :5.000   Max.   :9.000  \n       E        \n Min.   :1.000  \n 1st Qu.:3.500  \n Median :4.000  \n Mean   :4.407  \n 3rd Qu.:5.000  \n Max.   :8.000  \n\n\n\nsummary(mydata$A) # Summary information for column A \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.000   4.000   4.296   5.000   8.000 \n\n\n\nmean(mydata$A) \n\n[1] 4.296296\n\n\nTry some other functions like mean, median, min and max.\n\norder(mydata$A) #  The order function sorts a vector.\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25\n[26] 26 27\n\n\nWe can access specific rows, columns and cells within a table using square brackets: TABLE[ROW,COLUMN]. Try the following commands.\n\n##Print the value in the first column of the first row\nmydata[1,1]\n##Use blanks to print an entire row or column\nmydata[2,]\nmydata[,3]\n##You can select multiple rows and columns with ranges (:) or the c() function\nmydata[1:5,c(3,5)]\n##You can also use row or column names\nmydata[,\"B\"]\n##You can select rows or columns based on criteria (subsetting). \nmydata[mydata$B&gt;7,]\n\n\n\n Challenge:\n\n\nSee if you can do the following:\n\nSelect the 11th value in the third column\nSelect all rows where D equals 4 (hint; use ‘==’)\nSelect rows where B has its maximum value (hint: use the max function)\nSelect even numbered rows only (hint: take a look at the seq function ‘?seq()’)\nSelect columns A, C and E\nSort table by decreasing order of column B (hint: look at the options in the order function)\n\n\n\n\n\nSolution. \n\n Solution:\n\n\nSelect row 11, column 3\n\n\nmydata[11,3]\n\n[1] 1\n\n\n\nSelect rows where D 3 equals 4\n\n\nmydata[mydata$D==4,] \n\n   A B C D E\n19 5 8 2 4 5\n20 5 8 2 4 5\n21 5 8 2 4 5\n\n\n\nSelect rows where B has its maximum value\n\n\nmydata[mydata$B==max(mydata$B), ]\n\n   A  B C D E\n27 8 11 5 9 8\n\n\n\nSelect even numbered rows only\n\n\nmydata[seq(2,26, by = 2), ]\n\n   A  B C D E\n2  2  5 1 1 2\n4  3  6 1 2 3\n6  3  6 1 2 4\n8  3  6 1 2 3\n10 4  7 1 2 4\n12 4  7 1 2 5\n14 4  7 1 3 4\n16 4  7 1 3 4\n18 5  8 2 3 5\n20 5  8 2 4 5\n22 6  9 3 5 6\n24 6  9 3 6 6\n26 7 10 4 8 7\n\n\n\nSelect columns A, C and E\n\n\nmydata[, c(1,3,5)] \n\n   A C E\n1  1 1 1\n2  2 1 2\n3  2 1 2\n4  3 1 3\n5  3 1 3\n6  3 1 4\n7  3 1 3\n8  3 1 3\n9  4 1 4\n10 4 1 4\n11 4 1 4\n12 4 1 5\n13 4 1 5\n14 4 1 4\n15 4 1 4\n16 4 1 4\n17 4 2 4\n18 5 2 5\n19 5 2 5\n20 5 2 5\n21 5 2 5\n22 6 3 6\n23 6 3 6\n24 6 3 6\n25 7 4 7\n26 7 4 7\n27 8 5 8\n\n# Or mydata[,c('A','C','E')] \n\n\nSort table by decreasing order of column B\n\n\nmydata[order(mydata$B, decreasing = TRUE), ]\n\n   A  B C D E\n27 8 11 5 9 8\n25 7 10 4 7 7\n26 7 10 4 8 7\n22 6  9 3 5 6\n23 6  9 3 5 6\n24 6  9 3 6 6\n18 5  8 2 3 5\n19 5  8 2 4 5\n20 5  8 2 4 5\n21 5  8 2 4 5\n9  4  7 1 2 4\n10 4  7 1 2 4\n11 4  7 1 2 4\n12 4  7 1 2 5\n13 4  7 1 2 5\n14 4  7 1 3 4\n15 4  7 1 3 4\n16 4  7 1 3 4\n17 4  7 2 3 4\n4  3  6 1 2 3\n5  3  6 1 2 3\n6  3  6 1 2 4\n7  3  6 1 2 3\n8  3  6 1 2 3\n2  2  5 1 1 2\n3  2  5 1 1 2\n1  1  4 1 1 1\n\n\n\n\n\n\n\nThere is a subset() function in R specifically for filtering tables. This generally works better than using square brackets as it copes well with NA and NULL values.\n\nsubset(mydata,mydata$C==3)\n\n   A B C D E\n22 6 9 3 5 6\n23 6 9 3 5 6\n24 6 9 3 6 6\n\n\nThe tidyverse packages have their own set of functions for filtering data and we will explore these later."
  },
  {
    "objectID": "02-IntroR.html#plotting-with-r",
    "href": "02-IntroR.html#plotting-with-r",
    "title": "Introduction",
    "section": "Plotting with R",
    "text": "Plotting with R\nWe recommend learning ggplot2 for graphics but it is useful to know the options available in “base” R. Remember, to get more information about the options available to a function, type ?function.\n\nHistograms\n\nhist(mydata$A)\n\n\n\n\n\n\n\n\nThe ChickWeight data set is another data frame built into R. Using this larger set of data we can modify the number of vertical columns in a histogram with the option breaks.\n\nhist(ChickWeight$weight, breaks=5)\n\n\n\n\n\n\n\n\n\nhist(ChickWeight$weight, breaks=50)\n\n\n\n\n\n\n\n\n\n\nBoxplots\n\nboxplot(mydata)\n\n\n\n\n\n\n\n\n\nboxplot(mydata$A, mydata$B, names=c(\"Value A\", \"Value B\") , ylab=\"Count of Something\")\n\n\n\n\n\n\n\n\nWe can get rid of the need to type the data frame each time by using the attach function\n\nattach(mydata)\nboxplot(A, B, names=c(\"Value A\", \"Value B\") , ylab=\"Count of Something\")\n\n\n\n\n\n\n\n\nNote that the opposite function of attach is detach\n\ndetach(mydata)\n\n\n\nScatter plots\n\nattach(mydata) # Re-attach if needed\n\n\nplot(A,B)  # i.e. plot(mydata$A, mydata$B)"
  },
  {
    "objectID": "02-IntroR.html#saving-images",
    "href": "02-IntroR.html#saving-images",
    "title": "Introduction",
    "section": "Saving images",
    "text": "Saving images\nThere are a few ways to save images in RStudio:\n\nUse the export button in the Plots pane in Rstudio.\nUse a graphics device function in your R code.\n\nUse the png function to save a png file (easy to load into web applications and presentations).\n\npng(\"filename.png\") \nboxplot(A, B, names=c(\"Value A\", \"Value B\") , ylab=\"Count of Something\")\ndev.off()\n\nThe dev.off() function closes the graphics device. In the code above, everything between png() and dev.off() is saved to ‘filename.png’.\nYou can also save as a pdf.\n\npdf(\"filename.pdf\") \nboxplot(A, B, names=c(\"Value A\", \"Value B\") , ylab=\"Count of Something\")\ndev.off()"
  },
  {
    "objectID": "02-IntroR.html#statistical-testing",
    "href": "02-IntroR.html#statistical-testing",
    "title": "Introduction",
    "section": "Statistical testing",
    "text": "Statistical testing\nR has many functions for statistical testing.\n\n\n Further Resources\n\nPlease see our introduction to statistics document for more information on distributions, hypothesis testing and statistical significance.\n\n\nDifference between means\nLet’s say we want to determine whether the means of two groups of data differ statistically. First, we need to know if we are dealing with parametric or non-parametric data to choose the appropriate test i.e. are they normally distributed?\n\n\n Challenge:\n\nPlot a histogram for each vector in mydata to visualise the distributions of each dataset.\n\n\n\n\nSolution. \n\n Solution:\n\n\nhist(A, breaks=5)\n\n\n\n\n\n\n\nhist(B, breaks=5)\n\n\n\n\n\n\n\nhist(C, breaks=5)\n\n\n\n\n\n\n\nhist(D, breaks=5)\n\n\n\n\n\n\n\nhist(E, breaks=5)\n\n\n\n\n\n\n\n\n\n\n\n\nHow does the data look? Do any datasets appear to be normally distributed? We can test for normality with the Shapiro Wilk test. Let’s do this for column A:\n\nshapiro.test(A)\n\n\n    Shapiro-Wilk normality test\n\ndata:  A\nW = 0.95663, p-value = 0.3088\n\n\nThe null hypothesis in the Shapiro Wilk test is that our data does not differ significantly from a normal distribution. So, a significant p-value (p &lt; 0.05) means that the data is NOT normally distributed. As p here = 0.3088 (&gt; 0.05), we conclude that A is normally distributed and can be used in a parametric test.\n\n\n Challenge:\n\nWhich other columns are normally distributed?\n\n\n\n\nSolution. \n\n Solution:\n\n\nshapiro.test(A)\n\n\n    Shapiro-Wilk normality test\n\ndata:  A\nW = 0.95663, p-value = 0.3088\n\nshapiro.test(B)\n\n\n    Shapiro-Wilk normality test\n\ndata:  B\nW = 0.95663, p-value = 0.3088\n\nshapiro.test(C)\n\n\n    Shapiro-Wilk normality test\n\ndata:  C\nW = 0.71936, p-value = 7.294e-06\n\nshapiro.test(D)\n\n\n    Shapiro-Wilk normality test\n\ndata:  D\nW = 0.84078, p-value = 0.0007608\n\nshapiro.test(E)\n\n\n    Shapiro-Wilk normality test\n\ndata:  E\nW = 0.96665, p-value = 0.5162\n\n\nA, B & E are parametric datasets.\n\n\n\n\n\n\n Key points\n\nYou should only use parametric tests for parametric data!\n\n\n\nT-Test\nThe assumption for a t-test is that both groups are sampled from normal distributions with approximately equal variance. We can only use this test if the data is normally distributed. As columns A and E are both normally distributed we can use a Two Sample t-test to test if the mean values are statistically different. Our null hypothesis is that the two means are equal, and the alternative is that they are not.\nIf our p-value is less than the significance level 0.05, we can reject the null hypothesis and accept the alternative hypothesis. In other words, we can conclude that the mean values of group A and E are significantly different.\n\n\n Challenge:\n\nUse a t-test to decide if the mean values of A and E differ more than expected by random chance. Hint: Use the help search to find the t-test function.\n\n\n\n\nSolution. \n\n Solution:\n\n\nt.test(A,E)\n\n\n    Welch Two Sample t-test\n\ndata:  A and E\nt = -0.25047, df = 51.997, p-value = 0.8032\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.0012865  0.7790643\nsample estimates:\nmean of x mean of y \n 4.296296  4.407407 \n\n\nThe p-value of the test is 0.8032, which is greater than the significance level 0.05. We can conclude that means of A and E are NOT significantly different. You can also save the result as an object and print the p-value:\n\na_e_result &lt;- t.test(A,E)\na_e_result$p.value\n\n[1] 0.8032119\n\n\n\n\n\n\n\n\n Discussion\n\nAre any other (parametric) columns significantly different from each other?\n\n\n\nNon-Parametric Testing\nWhat if we want to test non-parametric data? As D is NOT normally distributed we need to use a non-parametric test. Here we use the Mann-Whitney U test, also known as Wilcoxon rank-sum test.\n\nwilcox.test(A,D) \n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  A and D\nW = 503.5, p-value = 0.01496\nalternative hypothesis: true location shift is not equal to 0\n\n\nNon parametric tests look at the ranks of values. If the same value appears multiple times in a dataset then the ranks will also be identical. R will output a warning if “tied ranks” exist. We can ignore it in this case but should be wary if there are many ties in our data.\nThe p-value of the test is 0.01496, which is less than the significance level alpha = 0.05. We can conclude that the mean of A is significantly different from the mean of D.\n\n\nPaired samples\nPaired data are sets of data from the same sample, e.g. a repeated measure or time series data. When looking at paired data, the pairs should exist in the same row of a data frame.\nLoad the paired dataset, ‘weight.tsv’, which contains weights of mice before and after a given treatment.\n\nweight &lt;- read.table(\"http://bifx-core3.bio.ed.ac.uk/weight.tsv\", header=TRUE)\nweight\n\n   before after\n1   200.1 392.9\n2   190.9 393.2\n3   192.7 345.1\n4   213.0 393.0\n5   241.4 434.0\n6   196.9 427.9\n7   172.2 422.0\n8   185.5 383.9\n9   205.2 392.3\n10  193.7 352.2\n\n\nPlot the data.\n\nboxplot(weight)\n\n\n\n\n\n\n\n\nAgain, before testing we first check for normality. Here we are comparing the differences between groups (Weight after MINUS Weight before).\n\nweight$after - weight$before\n\n [1] 192.8 202.3 152.4 180.0 192.6 231.0 249.8 198.4 187.1 158.5\n\n\nWe can add this as a new column called ‘diff’ to our table\n\nweight$diff &lt;- weight$after - weight$before\nweight\n\n   before after  diff\n1   200.1 392.9 192.8\n2   190.9 393.2 202.3\n3   192.7 345.1 152.4\n4   213.0 393.0 180.0\n5   241.4 434.0 192.6\n6   196.9 427.9 231.0\n7   172.2 422.0 249.8\n8   185.5 383.9 198.4\n9   205.2 392.3 187.1\n10  193.7 352.2 158.5\n\n\n\nshapiro.test(weight$diff)\n\n\n    Shapiro-Wilk normality test\n\ndata:  weight$diff\nW = 0.94536, p-value = 0.6141\n\n\nLooks good, the p-value is greater than 0.05 implying that the distribution of the differences (d) are not significantly different from the normal distribution. In other words, we can assume normality.\nWe want to know if the weights before treatment are significantly different to the weights after. Let’s run the t-test. If the data is paired we use the option paired=true.\n\nt.test(weight$before, weight$after, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  weight$before and weight$after\nt = -20.883, df = 9, p-value = 6.2e-09\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -215.5581 -173.4219\nsample estimates:\nmean difference \n        -194.49 \n\n\nThe p-value is 6.2e-09 (&lt; = 0.05) so we can then reject the null hypothesis and conclude that the average weight of the mice after treatment is significantly different from the average weight before treatment.\nIn the case of a non parametric paired data set, use a paired Mann-Whitney Wilcoxon test aka Wilcoxon Signed Rank Test e.g wilcox.test(x, y, paired = TRUE).\n\n\n Discussion\n\nHave a look at the other parameters of the t.test() function. What do you think these do?\n\n\n\nMultiple Testing\nIf you run multiple statistical tests on the same data then the probability of finding your results changes and you must adjust your p-values to compensate. This is known as multiple testing correction.\nThe easiest method is to use the function p.adjust(x), where x is a list of p-values. There is a method parameter to choose between different correction parameters. e.g.\n\npvals &lt;- c(0.0001, 0.05, 0.001, 0.1, 0.1, 0.1, 0.1,0.5,0.5,0.5)\np.adjust(pvals)\n\n [1] 0.001 0.400 0.009 0.700 0.700 0.700 0.700 1.000 1.000 1.000\n\n\nIf you want to compare the means of multiple groups, you could use a different statistical method like the ANOVA test. ANOVA tests for differences in the means of multiple groups and is an extension of the t-test."
  },
  {
    "objectID": "02-IntroR.html#matrix-data-and-frequency-tests",
    "href": "02-IntroR.html#matrix-data-and-frequency-tests",
    "title": "Introduction",
    "section": "Matrix Data and Frequency Tests",
    "text": "Matrix Data and Frequency Tests\nA matrix consists of values from the same data class, structured into rows and columns. You can turn a data frame into a matrix using the as.matrix() function.\n\nmymatrix &lt;- as.matrix(mydata)\n\nOr create one from a vector using the matrix() function (See ?matrix).\n\nv &lt;- c(54,66,80,20)\ntwoBytwo &lt;- matrix(v, nrow=2)\n\nWhich gives:\n\ntwoBytwo\n\n     [,1] [,2]\n[1,]   54   80\n[2,]   66   20\n\n\nNow the matrix is saved and is called twoBytwo.\nNote: nrow specifies the number of rows (alternatively you can specify the number of columns by ncol). The default parameters fill the matrix one column at a time. The byrow argument instructs the function to fill the matrix one row at a time.\n\nChi-squared and Fisher’s Exact Tests (Count based data)\nWe can place count based data in a matrix to perform statistical tests. For instance, when observing the presence of a fluorescent marker in wild-type and mutant cells we want to know if there is a significant difference between the 2 cell types?\nWild type cells with marker present: 54\nWild type cells with marker absent: 66\nMutant cells with marker present: 80\nMutant cells with marker absent: 20\nFortunately we have this already in the twoBytwo matrix, and the values in v.\nYou can change the default column and row names with the colnames and rownames function:\n\ncolnames(twoBytwo) &lt;- c(\"WT\", \"Mut\")\n\n\nrownames(twoBytwo) &lt;- c(\"pres\", \"abs\")\n\n\ntwoBytwo\n\n     WT Mut\npres 54  80\nabs  66  20\n\n\nThe Chi-squared test function chisq.test() works on matrices. It is a type of likelihood ratio test:\n\nchisq.test(twoBytwo)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  twoBytwo\nX-squared = 26.612, df = 1, p-value = 2.486e-07\n\n\nAs the p-value 0.0000002486 is less than 0.05, we can reject the null hypothesis and conclude that these cell types are significantly different.\nA Fisher’s exact test is generally preferred over a Chi-squared test as it is more robust to low numbers. However, it can only be used to compare two groups, while a Chi-squared can also be used to compare three or more.\n\nfisher.test(twoBytwo)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  twoBytwo\np-value = 1.029e-07\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.1055182 0.3899188\nsample estimates:\nodds ratio \n 0.2061251 \n\n\n\n\n Final Challenge:\n\nIn this challenge we are going to perform statistical analysis to see if the weights of 10 rabbits increase after a hypothetical experimental treatment. First prepare the data:\n\n# The data set \n# Weight of the rabbit before treatment \nbefore &lt;-c(190.1, 190.9, 172.7, 213, 231.4,  \n           196.9, 172.2, 285.5, 225.2, 113.7) \n  \n# Weight of the rabbit after treatment \nafter &lt;-c(392.9, 313.2, 345.1, 393, 434,  \n          227.9, 422, 383.9, 392.3, 801.2) \n  \n# Create a data frame \nrabbits &lt;- data.frame(  \n  sample=c(1:10), ##Assign sampleIDs\n  before=before,\n  after=after\n)\n\n\nConsider the following:\n\nPlot the data first. What is the best way to visualise this?\nAre the values independent or paired?\nShould you use a parametric or non-parametric test?\nWhich test will you use?\nWhat is the alternative hypothesis?\nAre the groups significantly different?\nWhat is the confidence interval?\n\n\n\n\n\nSolution. \n\n Solution:\n\nVisualise:\n\nboxplot(rabbits$before,rabbits$after)\n\n\n\n\n\n\n\n\nTest for normality:\n\nshapiro.test(rabbits$after-rabbits$before)\n\n\n    Shapiro-Wilk normality test\n\ndata:  rabbits$after - rabbits$before\nW = 0.70287, p-value = 0.0009544\n\n\nWe reject the Null hypothesis that the difference in weights is normally distributed, so we must use a non parametric test.\nThe data is paired and our null hypothesis is that the weight after treatment is not greater than the weight before. We therefore have a one-sided test and use “greater” as our alternative hypothesis. We include the option to produce confidence intervals:\n\nwilcox.test(rabbits$after, rabbits$before, paired = TRUE,alternative = \"greater\",conf.int = T) \n\n\n    Wilcoxon signed rank exact test\n\ndata:  rabbits$after and rabbits$before\nV = 55, p-value = 0.0009766\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 122.3   Inf\nsample estimates:\n(pseudo)median \n         176.2 \n\n\n\n\n\n\n\n\n\n Further Learning\n\nFurther examples are available on the sthda website.\n\n\nThe rstatix package is useful for applying statistical tests on tables of data and is compatible with the Tidy data structures and pipes that we will learn later on in these lessons. Here is an example which you can return to later.\n\n## Use the ToothGrowth dataset built into R\nlibrary(rstatix)\nlibrary(ggpubr)\ndf &lt;- ToothGrowth\ndf$dose &lt;- as.factor(df$dose)\nhead(df)\n\n   len supp dose\n1  4.2   VC  0.5\n2 11.5   VC  0.5\n3  7.3   VC  0.5\n4  5.8   VC  0.5\n5  6.4   VC  0.5\n6 10.0   VC  0.5\n\n\nThe toothgrowth dataset contains the length of odontoblasts (teeth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods (orange juice or ascorbic acid).\n\n## Group the data by dose and run a t_test between the two groups for each dose.\nstat.test &lt;- df |&gt;\n  group_by(dose) |&gt;\n  t_test(len ~ supp) |&gt;\n  adjust_pvalue() |&gt;\n  add_significance(\"p.adj\")\nstat.test\n\n# A tibble: 3 × 11\n  dose  .y.   group1 group2    n1    n2 statistic    df       p   p.adj\n  &lt;fct&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.5   len   OJ     VC        10    10    3.17    15.0 0.00636 0.0127 \n2 1     len   OJ     VC        10    10    4.03    15.4 0.00104 0.00312\n3 2     len   OJ     VC        10    10   -0.0461  14.0 0.964   0.964  \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\n\n## Plot out the results and add adjusted p-value\nggboxplot(\n  df, x = \"supp\", y = \"len\",\n  color = \"supp\", palette = \"jco\", facet.by = \"dose\",\n  ylim = c(0, 40)\n  ) +\n  stat_pvalue_manual(stat.test, label = \"p.adj\", y.position = 35)\n\n\n\n\n\n\n\n\n\n\n Key points\n\n\nR is a functional programming language\nRStudio is an interactive environment for programming in R\nBase R functions can be used to import, manipulate and plot data\nThere are many functions for statistical analysis in R"
  },
  {
    "objectID": "02-IntroR.html#consider-the-following",
    "href": "02-IntroR.html#consider-the-following",
    "title": "Introduction",
    "section": "Consider the following:",
    "text": "Consider the following:\n\nPlot the data first. What is the best way to visualise this?\nAre the values independent or paired?\nShould you use a parametric or non-parametric test?\nWhich test will you use?\nWhat is the alternative hypothesis?\nAre the groups significantly different?\nWhat is the confidence interval?\n\n\n\n\n\nSolution. \n\n Solution:\n\nVisualise:\n\nboxplot(rabbits$before,rabbits$after)\n\n\n\n\n\n\n\n\nTest for normality:\n\nshapiro.test(rabbits$after-rabbits$before)\n\n\n    Shapiro-Wilk normality test\n\ndata:  rabbits$after - rabbits$before\nW = 0.70287, p-value = 0.0009544\n\n\nWe reject the Null hypothesis that the difference in weights is normally distributed, so we must use a non parametric test.\nThe data is paired and our null hypothesis is that the weight after treatment is not greater than the weight before. We therefore have a one-sided test and use “greater” as our alternative hypothesis. We include the option to produce confidence intervals:\n\nwilcox.test(rabbits$after, rabbits$before, paired = TRUE,alternative = \"greater\",conf.int = T) \n\n\n    Wilcoxon signed rank exact test\n\ndata:  rabbits$after and rabbits$before\nV = 55, p-value = 0.0009766\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 122.3   Inf\nsample estimates:\n(pseudo)median \n         176.2"
  }
]